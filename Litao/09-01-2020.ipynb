{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "DIR = '/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/MD589/preps/CH1/full_aligned'\n",
    "volume = []\n",
    "for filename in sorted(os.listdir(DIR)):\n",
    "    filepath = os.path.join(DIR, filename)\n",
    "    volume.append(io.imread(filepath))\n",
    "volume = np.array(volume)\n",
    "volume = np.swapaxes(volume, 0, 2)\n",
    "\n",
    "# Precomputed folder path\n",
    "precompute_path = os.path.join(os.path.expanduser('~'), f'image_test')\n",
    "\n",
    "# Voxel resolution in nanometer (how much nanometer each element in numpy array represent)\n",
    "resol = (452, 452, 20000)\n",
    "\n",
    "# Voxel offset\n",
    "offset = (0, 0, 0)\n",
    "\n",
    "# Layer type\n",
    "layer_type = 'image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1366, 1012, 447)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(os.path.join(os.getcwd(), '../'))\n",
    "from utilities.sqlcontroller import SqlController\n",
    "\n",
    "DIR = os.path.join(os.path.expanduser('~'))\n",
    "animal = 'MD589'\n",
    "downsample_factor = 32\n",
    "\n",
    "volume_path = os.path.join(DIR, f'{animal}_annotations_down{downsample_factor}.npy')\n",
    "\n",
    "# Numpy array whose shape should match (x, y, z, channel) or just (x, y, z) if it's single channel\n",
    "with open(volume_path, 'rb') as file:\n",
    "    volume = np.load(file)\n",
    "print(volume.shape)\n",
    "\n",
    "# Precomputed folder path\n",
    "precompute_path = os.path.join(DIR, f'{animal}_annotations_down{downsample_factor}')\n",
    "\n",
    "# Voxel resolution in nanometer (how much nanometer each element in numpy array represent)\n",
    "resol = (14464, 14464, 20000)\n",
    "\n",
    "# Voxel offset\n",
    "offset = (0, 0, 0)\n",
    "\n",
    "# Layer type\n",
    "layer_type = 'segmentation'\n",
    "\n",
    "# segmentation properties in the format of [(number1, label1), (number2, label2) ...]\n",
    "# where number is an integer that is in the volume and label is a string that describes that segmenetation\n",
    "sqlController = SqlController('MD589')\n",
    "db_structures = sqlController.get_structures_dict()\n",
    "\n",
    "segmentation_properties = [(number, f'{structure}: {label}') for structure, (label, number, sided) in db_structures.items()]\n",
    "extra_structures = ['MVePC', 'DTgP', 'VTA', 'Li', 'Op', 'RPC', 'MVeMC', 'APT', 'IPR', 'Cb', \n",
    "                    'pc', 'SolIM', 'Pr5VL', 'IPC', '8n', 'MPB', 'Pr5', 'DRD', 'VTg', 'IF', \n",
    "                    'RR', 'LDTg', '5TT', 'Bar', 'IO', 'Cu', 'SuVe', 'PTg', 'MnR', 'Gr', \n",
    "                    'ECu', 'DTgC', 'IPA', 'LPB', 'EW', 'Pr5DM', 'Dk', 'DTg', 'LVe', 'SpVe', \n",
    "                    'MVe', 'LSO', 'InC', 'RMC', 'PF', 'CnF', 'Sol', 'IPL', 'X', 'MiTg', 'DRI', 'RPF']\n",
    "segmentation_properties += [(len(db_structures) + index + 1, structure) for index, structure in enumerate(extra_structures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:36395/v/aedbc3f54e88332d3fd58c3c48531753d216de2d/\n"
     ]
    }
   ],
   "source": [
    "# Run this cell if you want to preview the numpy array before converting to precomputed format\n",
    "import neuroglancer\n",
    "viewer = neuroglancer.Viewer()\n",
    "print(viewer)\n",
    "\n",
    "all_volume_layer = neuroglancer.SegmentationLayer(\n",
    "    source = neuroglancer.LocalVolume(\n",
    "        data=volume, \n",
    "        dimensions=neuroglancer.CoordinateSpace(names=['x', 'y', 'z'], units='nm', scales=resol), \n",
    "        voxel_offset=offset\n",
    "    ),\n",
    ")\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    s.layers.clear()\n",
    "    s.layers['all'] = all_volume_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 2464/2464 [00:11<00:00, 210.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/seung-lab/cloud-volume\n",
    "# Don't change all compress=False. It seems that Neuroglancer can only read with compress=False\n",
    "from cloudvolume import CloudVolume\n",
    "\n",
    "cloudpath = f'file://{precompute_path}'\n",
    "info = CloudVolume.create_new_info(\n",
    "    num_channels = volume.shape[3] if len(volume.shape) > 3 else 1,\n",
    "    layer_type = layer_type,\n",
    "    data_type = str(volume.dtype), # Channel images might be 'uint8'\n",
    "    encoding = 'raw', # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "    resolution = resol, # Voxel scaling, units are in nanometers\n",
    "    voxel_offset = offset, # x,y,z offset in voxels from the origin\n",
    "    chunk_size = [64, 64, 64], # units are voxels\n",
    "    volume_size = volume.shape[:3], # e.g. a cubic millimeter dataset\n",
    ")\n",
    "vol = CloudVolume(cloudpath, mip=0, info=info, compress=False)\n",
    "vol.commit_info()\n",
    "vol[:, :, :] = volume[:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "vol.info['segment_properties'] = 'names'\n",
    "vol.commit_info()\n",
    "\n",
    "segment_properties_path = os.path.join(precompute_path, 'names')\n",
    "os.makedirs(segment_properties_path, exist_ok=True)\n",
    "\n",
    "info = {\n",
    "    \"@type\": \"neuroglancer_segment_properties\", \n",
    "    \"inline\": {\n",
    "        \"ids\": [str(number) for number, label in segmentation_properties],\n",
    "        \"properties\": [{\n",
    "            \"id\": \"label\", \n",
    "            \"description\": \"Name of structures\",\n",
    "            \"type\": \"label\",\n",
    "            \"values\": [str(label) for number, label in segmentation_properties]\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(segment_properties_path, 'info'), 'w') as file:\n",
    "    json.dump(info, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/seung-lab/igneous\n",
    "from taskqueue import LocalTaskQueue\n",
    "import igneous.task_creation as tc\n",
    "\n",
    "tq = LocalTaskQueue(parallel=True)\n",
    "tasks = tc.create_downsampling_tasks(cloudpath, compress=False) # Downsample the volumes \n",
    "tq.insert(tasks)\n",
    "tq.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = tc.create_meshing_tasks(cloudpath, mip=0, compress=False) # The first phase of creating mesh\n",
    "tq.insert(tasks)\n",
    "tq.execute()\n",
    "\n",
    "# It should be able to incoporated to above tasks, but it will give a weird bug. Don't know the reason\n",
    "tasks = tc.create_mesh_manifest_tasks(cloudpath) # The second phase of creating mesh\n",
    "tq.insert(tasks)\n",
    "tq.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
