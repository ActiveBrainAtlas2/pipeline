{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting dklab@db.dk.ucsd.edu:3306\n",
      "http://127.0.0.1:35653/v/142ad5020879f6099b20e09caf1b294a25ec76b6/\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import subprocess\n",
    "from multiprocessing.pool import Pool\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "import ast\n",
    "from skimage import io\n",
    "from skimage.color import gray2rgb\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import neuroglancer\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '../'))\n",
    "from utilities.file_location import FileLocationManager\n",
    "from utilities.alignment_utility import load_consecutive_section_transform, convert_resolution_string_to_um\n",
    "\n",
    "viewer = neuroglancer.Viewer()\n",
    "print(viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animal = 'MD594'\n",
    "# aligned_shape = np.array((47000, 23300)) // 32\n",
    "# num_section = 433\n",
    "# animal = 'MD585'\n",
    "# aligned_shape = np.array((38200, 28000)) // 32\n",
    "# num_section = 446\n",
    "animal = 'MD589'\n",
    "aligned_shape = np.array((43700, 32400)) // 32\n",
    "num_section = 447\n",
    "\n",
    "color_filepath = os.path.join('../', 'neuroglancer/contours/json_cache', 'struct_to_color_2.json')\n",
    "with open(color_filepath, 'r') as json_file:\n",
    "    colors = json.load(json_file)\n",
    "colors = {name.upper(): index for name, index in colors.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the annotation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:07<00:00,  9.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dense_coordinates(coor_list):\n",
    "    dense_coor_list = []\n",
    "    # Shortest distance, x, y\n",
    "\n",
    "    # for x, y in coor_list:\n",
    "    for i in range(len(coor_list) - 1):\n",
    "        x, y = coor_list[i]\n",
    "        x_next, y_next = coor_list[i + 1]\n",
    "\n",
    "        x_mid = (x + x_next) / 2\n",
    "        y_mid = (y + y_next) / 2\n",
    "\n",
    "        dense_coor_list.append([x, y])\n",
    "        dense_coor_list.append([x_mid, y_mid])\n",
    "\n",
    "        if i == len(coor_list) - 2:\n",
    "            dense_coor_list.append([x_next, y_next])\n",
    "            x, y = coor_list[0]\n",
    "            x_mid = (x + x_next) / 2\n",
    "            y_mid = (y + y_next) / 2\n",
    "            dense_coor_list.append([x_mid, y_mid])\n",
    "\n",
    "    return dense_coor_list\n",
    "\n",
    "def get_contours_from_annotations(stack, target_structure, hand_annotations, densify=0):\n",
    "    MD585_ng_section_min = 83\n",
    "    num_annotations = len(hand_annotations)\n",
    "    str_contours_annotation = {}\n",
    "\n",
    "    for i in range(num_annotations):\n",
    "        structure = hand_annotations['name'][i]\n",
    "        side = hand_annotations['side'][i]\n",
    "        section = hand_annotations['section'][i]\n",
    "        first_sec = 0\n",
    "        last_sec = 0\n",
    "\n",
    "        #if side == 'R' or side == 'L':\n",
    "        #    structure = structure + '_' + side\n",
    "\n",
    "        if structure == target_structure:\n",
    "            vertices = hand_annotations['vertices'][i]\n",
    "\n",
    "            for i in range(densify):\n",
    "                vertices = get_dense_coordinates(vertices)\n",
    "\n",
    "            # Skip sections before the 22nd prep2 section for MD585 as there are clear errors\n",
    "            if stack == 'MD585' and section < MD585_ng_section_min + 22:\n",
    "                # vertices = vertices - np.array(MD585_abberation_correction)\n",
    "                continue\n",
    "            str_contours_annotation[section] = {}\n",
    "            str_contours_annotation[section][structure] = {}\n",
    "            str_contours_annotation[section][structure][1] = vertices\n",
    "\n",
    "    try:\n",
    "        first_sec = np.min(list(str_contours_annotation.keys()))\n",
    "        last_sec = np.max(list(str_contours_annotation.keys()))\n",
    "    except:\n",
    "        pass\n",
    "    return str_contours_annotation, first_sec, last_sec\n",
    "\n",
    "CSV_PATH = '/net/birdstore/Active_Atlas_Data/data_root/atlas_data/foundation_brain_annotations'\n",
    "csvfile = os.path.join(CSV_PATH, f'{animal}_annotation.csv')\n",
    "hand_annotations = pd.read_csv(csvfile)\n",
    "hand_annotations['vertices'] = hand_annotations['vertices'] \\\n",
    "    .apply(lambda x: x.replace(' ', ','))\\\n",
    "    .apply(lambda x: x.replace('\\n',','))\\\n",
    "    .apply(lambda x: x.replace(',]',']'))\\\n",
    "    .apply(lambda x: x.replace(',,', ','))\\\n",
    "    .apply(lambda x: x.replace(',,', ','))\\\n",
    "    .apply(lambda x: x.replace(',,', ',')).apply(lambda x: x.replace(',,', ','))\n",
    "hand_annotations['vertices'] = hand_annotations['vertices'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "structures = list(hand_annotations['name'].unique())\n",
    "section_structure_vertices = defaultdict(dict)\n",
    "for structure in tqdm(structures):\n",
    "    contour_annotations, first_sec, last_sec = get_contours_from_annotations(animal, structure, hand_annotations, densify=4)\n",
    "    for section in contour_annotations:\n",
    "        section_structure_vertices[section][structure] = contour_annotations[section][structure][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce create_clean transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 447/447 [00:17<00:00, 25.99it/s]\n"
     ]
    }
   ],
   "source": [
    "PATH = f'/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/{animal}/preps/CH1'\n",
    "thumbnail_dir = os.path.join(PATH, 'thumbnail')\n",
    "\n",
    "section_offset = {}\n",
    "for file_name in tqdm(sorted(os.listdir(thumbnail_dir))):\n",
    "    filepath = os.path.join(thumbnail_dir, file_name)\n",
    "    img = io.imread(filepath)\n",
    "    section = int(file_name.split('.')[0])\n",
    "    section_offset[section] = (aligned_shape - img.shape[:2][::-1]) // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce create_alignment transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_elastix(animal):\n",
    "    \"\"\"\n",
    "    After the elastix job is done, this goes into each subdirectory and parses the Transformation.0.txt file\n",
    "    Args:\n",
    "        animal: the animal\n",
    "    Returns: a dictionary of key=filename, value = coordinates\n",
    "    \"\"\"\n",
    "    fileLocationManager = FileLocationManager(animal)\n",
    "    DIR = fileLocationManager.prep\n",
    "    INPUT = os.path.join(DIR, 'CH1', 'thumbnail_cleaned')\n",
    "\n",
    "    image_name_list = sorted(os.listdir(INPUT))\n",
    "    anchor_idx = len(image_name_list) // 2\n",
    "    # anchor_idx = len(image_name_list) - 1\n",
    "    transformation_to_previous_sec = {}\n",
    "\n",
    "    for i in range(1, len(image_name_list)):\n",
    "        fixed_fn = os.path.splitext(image_name_list[i - 1])[0]\n",
    "        moving_fn = os.path.splitext(image_name_list[i])[0]\n",
    "        transformation_to_previous_sec[i] = load_consecutive_section_transform(animal, moving_fn, fixed_fn)\n",
    "\n",
    "    transformation_to_anchor_sec = {}\n",
    "    # Converts every transformation\n",
    "    for moving_idx in range(len(image_name_list)):\n",
    "        if moving_idx == anchor_idx:\n",
    "            transformation_to_anchor_sec[image_name_list[moving_idx]] = np.eye(3)\n",
    "        elif moving_idx < anchor_idx:\n",
    "            T_composed = np.eye(3)\n",
    "            for i in range(anchor_idx, moving_idx, -1):\n",
    "                T_composed = np.dot(np.linalg.inv(transformation_to_previous_sec[i]), T_composed)\n",
    "            transformation_to_anchor_sec[image_name_list[moving_idx]] = T_composed\n",
    "        else:\n",
    "            T_composed = np.eye(3)\n",
    "            for i in range(anchor_idx + 1, moving_idx + 1):\n",
    "                T_composed = np.dot(transformation_to_previous_sec[i], T_composed)\n",
    "            transformation_to_anchor_sec[image_name_list[moving_idx]] = T_composed\n",
    "\n",
    "\n",
    "    return transformation_to_anchor_sec\n",
    "\n",
    "def convert_2d_transform_forms(arr):\n",
    "    return np.vstack([arr, [0,0,1]])\n",
    "\n",
    "def create_warp_transforms(animal, transforms, transforms_resol, resolution):\n",
    "    #transforms_resol = op['resolution']\n",
    "    transforms_scale_factor = convert_resolution_string_to_um(animal, resolution=transforms_resol) / convert_resolution_string_to_um(animal, resolution=resolution)\n",
    "    tf_mat_mult_factor = np.array([[1, 1, transforms_scale_factor], [1, 1, transforms_scale_factor]])\n",
    "    transforms_to_anchor = {\n",
    "        img_name:\n",
    "            convert_2d_transform_forms(np.reshape(tf, (3, 3))[:2] * tf_mat_mult_factor) for\n",
    "        img_name, tf in transforms.items()}\n",
    "\n",
    "    return transforms_to_anchor\n",
    "\n",
    "transforms = parse_elastix(animal)\n",
    "warp_transforms = create_warp_transforms(animal, transforms, 'thumbnail', 'thumbnail')\n",
    "ordered_transforms = sorted(warp_transforms.items())\n",
    "\n",
    "section_transform = {}\n",
    "for section, transform in ordered_transforms:\n",
    "    section_num = int(section.split('.')[0])\n",
    "    transform = np.linalg.inv(transform)\n",
    "    section_transform[section_num] = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment of annotation coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(x', y') = (x * sx + y * ry + tx, x * rx + y * sy + ty)\n",
    "'sx': T[0, 0], 'sy': T[1, 1], 'rx': T[1, 0], 'ry': T[0, 1], 'tx': T[0, 2], 'ty': T[1, 2]\n",
    "'''\n",
    "def transform_create_alignment(points, transform):\n",
    "    a = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    b = transform.T[:, 0:2]\n",
    "    c = np.matmul(a, b)\n",
    "    return c\n",
    "\n",
    "volume = np.zeros((aligned_shape[1], aligned_shape[0], num_section), dtype=np.uint8)\n",
    "for section in section_structure_vertices:\n",
    "    template = np.zeros((aligned_shape[1], aligned_shape[0]), dtype=np.uint8)\n",
    "    for structure in section_structure_vertices[section]:\n",
    "        points = np.array(section_structure_vertices[section][structure])\n",
    "        points = points // 32\n",
    "        points = points + section_offset[section] # create_clean offset\n",
    "        points = transform_create_alignment(points, section_transform[section]) # create_alignment transform\n",
    "        points = points.astype(np.int32)\n",
    "        \n",
    "        try:\n",
    "            color = colors[structure.upper()]\n",
    "        except:\n",
    "            sided = '{}_R'.format(structure)\n",
    "            try:\n",
    "                color = colors[sided]\n",
    "            except:\n",
    "                color = 255\n",
    "            \n",
    "        cv2.polylines(template, [points], True, color, 2, lineType=cv2.LINE_AA)\n",
    "    volume[:, :, section - 1] = template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = np.swapaxes(volume, 0, 1)\n",
    "all_volume_layer = neuroglancer.SegmentationLayer(\n",
    "    source = neuroglancer.LocalVolume(\n",
    "        data=volume, \n",
    "        dimensions=neuroglancer.CoordinateSpace(names=['x', 'y', 'z'], units='nm', scales=[14464, 14464, 20000]), \n",
    "        voxel_offset=(0, 0, 0)\n",
    "    ),\n",
    ")\n",
    "\n",
    "with viewer.txn() as s:\n",
    "    s.layers.clear()\n",
    "    s.layers['all'] = all_volume_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{animal}_annotations.npy', 'wb') as file:\n",
    "    np.save(file, volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
