{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser(\"~\")\n",
    "PATH = os.path.join(HOME, 'programming/pipeline_utility/src')\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utilities_alignment import (create_warp_transforms, parse_elastix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = 'DK55'\n",
    "downsample_factor = 32\n",
    "CSV_PATH = f'/net/birdstore/Active_Atlas_Data/data_root/atlas_data/{animal}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = f'/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/{animal}/preps'\n",
    "cshl_csvfile = 'cshl.premotor.csv'\n",
    "cshl_csvpath = os.path.join(CSV_PATH, cshl_csvfile)\n",
    "cshl_df = pd.read_csv(cshl_csvpath, names=['section','x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = cshl_df['section'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_size = {}\n",
    "for section in sections:\n",
    "    filename = str(section).zfill(3) + '.tif'\n",
    "    filepath = os.path.join(IMG_PATH, 'CH3/full', filename)\n",
    "    input_image = Image.open(filepath)\n",
    "    rotated_height = input_image.width\n",
    "    section_size[section] = rotated_height\n",
    "    input_image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in cshl_df.iterrows():\n",
    "    section = row['section']\n",
    "    rotated_height = section_size[section]\n",
    "    cshl_df.at[index,'xp'] = cshl_df.at[index,'y'] \n",
    "    cshl_df.at[index,'yp'] = rotated_height - cshl_df.at[index,'x'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cshl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the annotation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_vertices = defaultdict(list)\n",
    "for index, row in cshl_df.iterrows():\n",
    "    section = row['section']\n",
    "    x = row['xp']\n",
    "    y = row['yp']\n",
    "    section_vertices[section].append([x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce create_clean transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the difference between the image stack size which is consistent for all images\n",
    "# and the individual shape of the aligned image: use thumbnail \n",
    "INPUT = f'/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/{animal}/preps/CH3/thumbnail'\n",
    "section_offset = {}\n",
    "\n",
    "# original image is long and narrow with the brain stem at the bottom\n",
    "width = 34000\n",
    "height = 60000\n",
    "downsample_factor = 32\n",
    "# these get switched here\n",
    "aligned_shape = np.array((width, height)) / downsample_factor\n",
    "print(aligned_shape)\n",
    "\n",
    "for file_name in sorted(os.listdir(INPUT)):\n",
    "    filepath = os.path.join(INPUT, file_name)    \n",
    "    input_image = Image.open(filepath)\n",
    "    width = input_image.width\n",
    "    height = input_image.height\n",
    "    downsampled_shape = np.array((width, height))\n",
    "    difference = np.round(aligned_shape - downsampled_shape)\n",
    "    section = int(file_name.split('.')[0])\n",
    "    #print(section, difference, width,height)\n",
    "    section_offset[section] = difference / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(section, downsample_factor, section_offset[224])\n",
    "#section_offset[224] = [68,66]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce create_alignment transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = parse_elastix(animal)\n",
    "downsample_factor = 32\n",
    "warp_transforms = create_warp_transforms(animal, transforms, downsample=True)\n",
    "ordered_transforms = sorted(warp_transforms.items())\n",
    "\n",
    "section_transform = {}\n",
    "for filename, transform in ordered_transforms:\n",
    "    section_num = int(filename.split('.')[0])\n",
    "    transform = np.linalg.inv(transform) keeping this makes it worse\n",
    "    section_transform[section_num] = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment of annotation coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(x', y') = (x * sx + y * ry + tx, x * rx + y * sy + ty)\n",
    "'sx': T[0, 0], 'sy': T[1, 1], 'rx': T[1, 0], 'ry': T[0, 1], 'tx': T[0, 2], 'ty': T[1, 2]\n",
    "'''\n",
    "def transform_create_alignment(points, transform):\n",
    "    a = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    b = transform.T[:, 0:2]\n",
    "    c = np.matmul(a, b)\n",
    "    return c\n",
    "\n",
    "aligned_section_structure_polygons = defaultdict(list)\n",
    "for section, vertices in section_vertices.items():\n",
    "    points = np.array(vertices)\n",
    "    points = points + section_offset[section] # create_clean offset\n",
    "    points = transform_create_alignment(points, section_transform[section]) # create_alignment transform\n",
    "    aligned_section_structure_polygons[section] = [points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for section,v in aligned_section_structure_polygons.items():\n",
    "    for x,y in v[0]:\n",
    "        data.append([x,y,section])    \n",
    "\n",
    "df = pd.DataFrame(data, columns=['x','y','section'])\n",
    "df = df.astype({'section':'int32','x': 'float64', 'y':'float64'})\n",
    "outfile = f'/net/birdstore/Active_Atlas_Data/data_root/atlas_data/DK55/cshl2dk.aligned.csv'\n",
    "#df.to_csv(outfile, index=False, header=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, aligned_section_structure_polygons variable contains the aligned polygon vertices for each structure in each section. \n",
    "From now on, we introduce how to draw these points to numpy array or neuroglancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section = 224\n",
    "filename = f'{str(section).zfill(3)}.tif'\n",
    "INPUT = '/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/DK55/preps/CH3'\n",
    "outpath = os.path.join(INPUT, '224.out.noinv.switchedoffsets.tif')\n",
    "filepath = os.path.join(INPUT, 'thumbnail_aligned_norm', filename)\n",
    "#filepath = os.path.join(INPUT, '224.norm.rotated.tif')\n",
    "img = cv2.imread(filepath, -1)\n",
    "#img = (img/256).astype(np.uint8)\n",
    "#clahe = cv2.createCLAHE(clipLimit=30.0, tileGridSize=(4, 4))\n",
    "#img = clahe.apply(img)\n",
    "\n",
    "radius = 5\n",
    "color = (0,255,1)\n",
    "#df = cshl_df.copy()\n",
    "df = df.loc[df['section'] == section]\n",
    "for index, row in df.iterrows():\n",
    "    x = round(row['x']/32) \n",
    "    y = round(row['y']/32)\n",
    "    #print(x,y)\n",
    "    cv2.circle(img, (int(x), int(y)), radius, color, 2)\n",
    "\n",
    "cv2.imwrite(outpath, img)\n",
    "fig=plt.figure(figsize=(26,18), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title('Aligned section:{}'.format(section), fontsize=30)\n",
    "plt.tick_params(axis='x', labelsize=30)\n",
    "plt.tick_params(axis='y', labelsize=30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
