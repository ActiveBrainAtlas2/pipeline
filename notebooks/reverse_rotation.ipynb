{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser(\"~\")\n",
    "PATH = os.path.join(HOME, 'programming/pipeline_utility/src')\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utilities_alignment import (create_warp_transforms, parse_elastix)\n",
    "def transform_create_alignment(points, transform):\n",
    "    a = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    b = transform.T[:, 0:2]\n",
    "    c = np.matmul(a, b)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = 'DK55'\n",
    "downsample_factor = 32\n",
    "CSV_PATH = f'/net/birdstore/Active_Atlas_Data/data_root/atlas_data/{animal}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = f'/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/{animal}/preps'\n",
    "cshl_csvfile = 'cshl.premotor.csv'\n",
    "cshl_csvpath = os.path.join(CSV_PATH, cshl_csvfile)\n",
    "cshl_df = pd.read_csv(cshl_csvpath, names=['section','x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = cshl_df['section'].unique().tolist()\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_size = {}\n",
    "for section_num in sections:\n",
    "    filename = str(section_num).zfill(3) + '.tif'\n",
    "    filepath = os.path.join(IMG_PATH, 'CH3/full', filename)\n",
    "    input_image = Image.open(filepath)\n",
    "    rotated_height = input_image.width\n",
    "    section_size[section_num] = rotated_height\n",
    "    input_image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in cshl_df.iterrows():\n",
    "    section_num = int(row['section'])\n",
    "    rotated_height = section_size[section_num]\n",
    "    cshl_df.at[index,'xp'] = cshl_df.at[index,'y'] \n",
    "    cshl_df.at[index,'yp'] = rotated_height - cshl_df.at[index,'x'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cshl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on unaligned, normalized and rotated images. These turn out nicely\n",
    "section_test_num = 228\n",
    "filename = f'{str(section_test_num).zfill(3)}.tif'\n",
    "filepath = os.path.join(IMG_PATH, 'CH3/normalized', filename)\n",
    "img = cv2.imread(filepath, -1)\n",
    "radius = 5\n",
    "color = (0,255,1)\n",
    "df = cshl_df.loc[cshl_df['section'] == section_test_num]\n",
    "for index, row in df.iterrows():\n",
    "    x = round(row['xp']/downsample_factor) \n",
    "    y = round(row['yp']/downsample_factor)\n",
    "    #print(x,y)\n",
    "    cv2.circle(img, (int(x), int(y)), radius, color, 2)\n",
    "\n",
    "outpath = os.path.join(IMG_PATH,'CH3',f'{section_test_num}.out.tif')\n",
    "cv2.imwrite(outpath, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the annotation points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_vertices = defaultdict(list)\n",
    "for index, row in cshl_df.iterrows():\n",
    "    section_num = int(row['section'])\n",
    "    x = row['xp']\n",
    "    y = row['yp']\n",
    "    section_vertices[section_num].append([x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce create_clean transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the difference between the image stack size which is consistent for all images\n",
    "# and the individual shape of the aligned image: use thumbnail \n",
    "INPUT = f'/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/{animal}/preps/CH3/thumbnail'\n",
    "section_offset = {}\n",
    "\n",
    "downsample_factor = 32\n",
    "# original image is long and narrow with the brain stem at the bottom\n",
    "# work with downsampled sizes\n",
    "fixed_width = 1875\n",
    "fixed_height = 1062\n",
    "#1875x1062\n",
    "# below is orig\n",
    "#aligned_shape = np.array((width, height)) / downsample_factor\n",
    "#below works\n",
    "cleaned_section_points = defaultdict(list)\n",
    "\n",
    "for section_num in sections:\n",
    "    filename = str(section_num).zfill(3) + '.tif'\n",
    "    filepath = os.path.join(INPUT, filename)    \n",
    "    input_image = Image.open(filepath)\n",
    "    xshift = (fixed_width - input_image.height) / 2 \n",
    "    yshift = (fixed_height - input_image.width) / 2\n",
    "    vertices = section_vertices[section_num]\n",
    "    points = np.array(vertices) / downsample_factor\n",
    "    shifts = np.array([xshift, yshift])\n",
    "    print(filename, shifts)\n",
    "    cleaned_section_points[section_num] = points + shifts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test create_clean transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on image that has been rotated and placed in standard size\n",
    "filename = f'{str(section_test_num).zfill(3)}.tif'\n",
    "filepath = os.path.join(IMG_PATH, 'CH3/thumbnail_cleaned_normalized', filename)\n",
    "img = cv2.imread(filepath, -1)\n",
    "points = cleaned_section_points[section_test_num]\n",
    "for point in points:\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    #print(x,y)\n",
    "    cv2.circle(img, (int(x), int(y)), radius, color, 2)\n",
    "outpath = os.path.join(IMG_PATH,'CH3',f'{section_test_num}.cleaned.out.tif')\n",
    "cv2.imwrite(outpath, img)\n",
    "fig=plt.figure(figsize=(26,18), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title('Placed section:{}'.format(section_test_num), fontsize=30)\n",
    "plt.tick_params(axis='x', labelsize=30)\n",
    "plt.tick_params(axis='y', labelsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce create_alignment transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = parse_elastix(animal)\n",
    "warp_transforms = create_warp_transforms(animal, transforms, downsample=True)\n",
    "ordered_transforms = sorted(warp_transforms.items())\n",
    "\n",
    "section_transform = {}\n",
    "for filename, transform in ordered_transforms:\n",
    "    section_num = int(filename.split('.')[0])\n",
    "    transform = np.linalg.inv(transform) \n",
    "    section_transform[section_num] = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment of annotation coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_section_points = defaultdict(list)\n",
    "aligned_section_points = defaultdict(list)\n",
    "for section_num, vertices in section_vertices.items():\n",
    "    points = np.array(vertices) / downsample_factor\n",
    "    cleaned_points = points + section_offset[section_num] # create_clean offset\n",
    "    cleaned_section_points[section_num] = cleaned_points\n",
    "    points = transform_create_alignment(cleaned_points, section_transform[section_num]) # create_alignment transform\n",
    "    aligned_section_points[section_num] = [points]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = cleaned_section_points[section_test_num]\n",
    "print(type(points), section_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for section,v in aligned_section.items():\n",
    "    for x,y in v[0]:\n",
    "        data.append([x,y,section])    \n",
    "\n",
    "df = pd.DataFrame(data, columns=['x','y','section'])\n",
    "df = df.astype({'section':'int32','x': 'float64', 'y':'float64'})\n",
    "outfile = f'/net/birdstore/Active_Atlas_Data/data_root/atlas_data/DK55/cshl2dk.aligned.csv'\n",
    "#df.to_csv(outfile, index=False, header=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, aligned_section_structure_polygons variable contains the aligned polygon vertices for each structure in each section. \n",
    "From now on, we introduce how to draw these points to numpy array or neuroglancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section = 222\n",
    "filename = f'{str(section).zfill(3)}.tif'\n",
    "INPUT = '/net/birdstore/Active_Atlas_Data/data_root/pipeline_data/DK55/preps/CH3'\n",
    "outpath = os.path.join(INPUT, f'{section}.out.tif')\n",
    "filepath = os.path.join(INPUT, 'thumbnail_aligned_norm', filename)\n",
    "#filepath = os.path.join(INPUT, '224.norm.rotated.tif')\n",
    "img = cv2.imread(filepath, -1)\n",
    "#img = (img/256).astype(np.uint8)\n",
    "#clahe = cv2.createCLAHE(clipLimit=30.0, tileGridSize=(4, 4))\n",
    "#img = clahe.apply(img)\n",
    "\n",
    "radius = 5\n",
    "color = (0,255,1)\n",
    "#df = cshl_df.copy()\n",
    "df = df.loc[df['section'] == section]\n",
    "for index, row in df.iterrows():\n",
    "    x = round(row['x']) \n",
    "    y = round(row['y'])\n",
    "    print(x,y)\n",
    "    cv2.circle(img, (int(x), int(y)), radius, color, 2)\n",
    "\n",
    "cv2.imwrite(outpath, img)\n",
    "fig=plt.figure(figsize=(26,18), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title('Aligned section:{}'.format(section), fontsize=30)\n",
    "plt.tick_params(axis='x', labelsize=30)\n",
    "plt.tick_params(axis='y', labelsize=30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
